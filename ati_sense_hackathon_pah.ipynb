{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "scheduled-survivor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\s2126908\\\\OneDrive\\\\Oceanography Coursework\\\\PhD\\\\Leeds_training\\\\w2\\\\2021.02.25.hackathon_ati'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####import json\n",
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sys import path\n",
    "#import time\n",
    "#from tqdm import tqdm\n",
    "import glob\n",
    "import rasterio\n",
    "\n",
    "%matplotlib inline\n",
    "os.getcwd()\n",
    "os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-technician",
   "metadata": {},
   "source": [
    "# Automated Sentinel-1 Ice, Water, Land Segmentation Challenge\n",
    "\n",
    "\n",
    "\n",
    "This notebook is the adjusted template, to help guide through the training process. Feel free to use as little or as much of it as you like.\n",
    "\n",
    "For the purposes of the template, we will assume a *classification* approach, which involves sub-sampling small images from the Sentinel-1 images. There will be notes where code should be adjusted for a *segmentation* approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-comparative",
   "metadata": {},
   "source": [
    "### Dataset preparation - (1) sub-sampling\n",
    "\n",
    "Sample patches from each TIF image, and find the corresponding label using the Shapefiles. Save each image with a unique ID save in the directory **SAMPLING_DIR**. Save the corresponding meta data in the following format (this could be a CSV file, NumPy array, or some other format), in the directory **META_DIR**:\n",
    "\n",
    "\n",
    "```\n",
    "image_id, x, y, label\n",
    "```\n",
    "\n",
    "Set the label value as one of \"L\", \"W\", \"I\" as specified in the Shapefiles.\n",
    "\n",
    "To make it easier to patch the final segmentation back together, it is suggested to use the (x, y) pixel coordinates of the patch, rather than the spatial coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "humanitarian-superintendent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s2126908\\OneDrive\\Oceanography Coursework\\PhD\\Leeds_training\\w2\\2021.02.25.hackathon_ati\\sampling_dir C:\\Users\\s2126908\\OneDrive\\Oceanography Coursework\\PhD\\Leeds_training\\w2\\2021.02.25.hackathon_ati\\meta_dir\n"
     ]
    }
   ],
   "source": [
    "curdir = os.path.dirname(os.getcwd())\n",
    "SAMPLING_DIR = curdir + '\\sampling_dir' \n",
    "META_DIR = curdir + '\\meta_dir'\n",
    "print(SAMPLING_DIR, META_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-butter",
   "metadata": {},
   "source": [
    "Some helpful code: reading in a single Sentinel-1 image and the corresponding Shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "earlier-hebrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s2126908\\OneDrive\\Oceanography Coursework\\PhD\\Leeds_training\\w2\\2021.02.25.hackathon_ati\\EE_Polar_Training_Dataset_v-1-0-0\\Sea_Ice\\seaice_s1_20180116t075430.shp\n",
      "20180116T075430\n",
      "C:\\Users\\s2126908\\OneDrive\\Oceanography Coursework\\PhD\\Leeds_training\\w2\\2021.02.25.hackathon_ati\\Sentinel geotiffs\\S1A_EW_GRDM_1SDH_20180116T075430_20180116T075530_020177_0226B9_9FE3_Orb_Cal_Spk_TC_rgb_8bit.tif\n"
     ]
    }
   ],
   "source": [
    "# the directory containing all shapefiles - i.e., the location of sea_ice/ \n",
    "SHAPEFILE_DIR = curdir + '\\EE_Polar_Training_Dataset_v-1-0-0'  \n",
    "\n",
    "shapefile = SHAPEFILE_DIR + '\\Sea_Ice\\seaice_s1_20180116t075430.shp' # full name of .shp file\n",
    "\n",
    "print(shapefile)\n",
    "# extract the shape ID, for example, 20180116T075430\n",
    "shp_id = shapefile.split(\"_\")[-1][:-4].upper()\n",
    "print(shp_id)\n",
    "\n",
    "# locate the corresponding Sentinel-1 image based on the ID\n",
    "# this should only return 1 match, which you can confirm\n",
    "tiff_file = curdir + '\\Sentinel geotiffs\\S1?_*_' + shp_id + '*.tif'\n",
    "#print(tiff_file)\n",
    "#S1B_EW_GRDM_1SDH_20181113T074529_20181113T074629_013583_019254_D382_Orb_Cal_Spk_TC_rgb_8bit\n",
    "\n",
    "#for name in glob.glob(tiff_file):\n",
    "#    print(name)\n",
    "\n",
    "tiff_file = glob.glob(tiff_file)[0]\n",
    "print(tiff_file)\n",
    "\n",
    "#tiff_file = #[g for g in tiff_files if shp_id in g]\n",
    "#tiff_file = tiff_file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-entity",
   "metadata": {},
   "source": [
    "Feel free to use other Python packages; but as an example, here we use **geopandas** to read in the Shapefile, and **rasterio** to read the GeoTIFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "figured-allergy",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-a992031fb22b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#shape_data = gpd.read_file(SHAPEFILE_DIR + shapefile)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mshape_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapefile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mshape_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gpd' is not defined"
     ]
    }
   ],
   "source": [
    "#shape_data = gpd.read_file(SHAPEFILE_DIR + shapefile)\n",
    "shape_data = gpd.read_file(shapefile)\n",
    "\n",
    "shape_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "returning-culture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<open DatasetReader name='C:/Users/s2126908/OneDrive/Oceanography Coursework/PhD/Leeds_training/w2/2021.02.25.hackathon_ati/Sentinel geotiffs/S1A_EW_GRDM_1SDH_20180116T075430_20180116T075530_020177_0226B9_9FE3_Orb_Cal_Spk_TC_rgb_8bit.tif' mode='r'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# directory containing all GeoTIFF files\n",
    "TIFF_DIR = curdir + '\\Sentinel geotiffs'  \n",
    "\n",
    "#tif_img = rasterio.open(TIFF_DIR + tiff_file)\n",
    "tif_img = rasterio.open(tiff_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-rochester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-austin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-legislation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "running-syntax",
   "metadata": {},
   "source": [
    "The shapes in the Shapefiles are **shapely** objects. We can also use the Python package **shapely** to check whether an x, y pixel coordinate position is in a given polyshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "confident-proceeding",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c3e8c8edbe13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mshape_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mshape_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'geometry'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshape_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Point\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"is in shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"and has class\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'poly_type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshape_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shape_data' is not defined"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "x = 4000\n",
    "y = 8000\n",
    "\n",
    "point = Point(x, y)\n",
    "\n",
    "# for example, specify the shape in the Shapefile\n",
    "shape_id = 2\n",
    "\n",
    "if shape_data['geometry'][shape_id].contains(point):\n",
    "    print(\"Point\", point, \"is in shape\", shape_id, \"and has class\", shape_data['poly_type'][shape_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-malta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-details",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "popular-usage",
   "metadata": {},
   "source": [
    "Define a train/validation ratio. Patches and meta saved from the test TIF images should be stored in separate directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "\n",
    "# valid size = 1.0 - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-canadian",
   "metadata": {},
   "source": [
    "Map the class category characters to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "\t\"L\": 0,\n",
    "\t\"W\": 1,\n",
    "\t\"I\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-addition",
   "metadata": {},
   "source": [
    "The following is a Dataset class which reads in image data saved in the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PolarPatch(Dataset):\n",
    "    def __init__(self, transform=None, split=\"train\"):\n",
    "        super(PolarPatch, self).__init__()\n",
    "\n",
    "        assert split in [\"train\", \"val\"]\n",
    "        \n",
    "        # TODO: load in meta data, which should be of shape (3, N) - N being the number of samples\n",
    "        meta = []\n",
    "\n",
    "        train_dim = int(TRAIN_SIZE * len(meta))\n",
    "        \n",
    "        if split == \"train\":\n",
    "            meta = meta[:train_dim]\n",
    "        else:\n",
    "            meta = meta[train_dim:]                   \n",
    "\n",
    "        self.images = range(len(meta))\n",
    "        self.coords = [(row[1], row[2]) for row in meta]\n",
    "\n",
    "        # Targets in integer form for computing cross entropy\n",
    "        self.targets = [LABELS[row[3]] for row in meta]\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        x = Image.open(SAMPLING_DIR + str(self.images[index]) + \".png\") # change this file format if needed\n",
    "        y = self.targets[index]\n",
    "        coord = self.coords[index]\n",
    "\n",
    "        if self.transform:\n",
    "        \tx = self.transform(x)\n",
    "\n",
    "        return x, y, coord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-excerpt",
   "metadata": {},
   "source": [
    "An example data transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    # TODO: add whatever else you need - normalisation, augmentation, etc.\n",
    "\ttransforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-spoke",
   "metadata": {},
   "source": [
    "### Dataset preparation - (2) data loaders\n",
    "\n",
    "Now we can prepare the data loaders. Here is the example for the training set; you will also need the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TODO set this value based on your working environment\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_set = PolarPatch(\n",
    "    split='train',\n",
    "    transform=data_transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-combine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "automotive-stock",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "You can use a custom model architecture, or copy one from literature. It is recommended to not build too deep of a network for the sake of training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PolarNet(nn.Module):\n",
    "    def __init__(self, n_classes=3):\n",
    "        super(PolarNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # TODO: build your own architecture here; one conv layer and ReLU here as an example only\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True), \n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # TODO: continue classifier section of architecture here for classification approach;\n",
    "            # otherwise, remove and add in upscaling for a fully-convolutional segmentation approach \n",
    "            nn.Linear(4096, n_classes),\n",
    "        )      \n",
    "\n",
    "    def forward(self, x):\n",
    "        # as an example; alter as needed depending on your architecture\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alpha-custom",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "An example of loading the model, setting a loss criteria and defining an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration - defaults to CPU unless GPU is available on device\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = PolarNet().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Stochastic gradient descent - TODO: alter as needed\n",
    "optimizer = optim.SGD(\n",
    "\tmodel.parameters(),\n",
    "\tlr=0.001,\n",
    "\tweight_decay=0.0005,\n",
    "\tmomentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-truth",
   "metadata": {},
   "source": [
    "Train the model, batch by batch, for as many iterations as required to converge. You can use the validation set to determine automatically when to stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-motor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-video",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluate patch-based accuracy on the test set; then using the test patch coordinates, piece together the segmentation prediction on the original TIF images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-printer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-heritage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-error",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-ambassador",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
